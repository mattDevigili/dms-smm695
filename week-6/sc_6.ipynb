{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T08:13:31.974944Z",
     "start_time": "2020-06-05T08:13:31.968492Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 6 - SMM695\n",
    "\n",
    "Matteo Devigili\n",
    "\n",
    "June, 30th 2023\n",
    "\n",
    "[_PySpark_](https://spark.apache.org/docs/latest/api/python/index.html#): during this lecture, we will approach Spark through Python\n",
    "\n",
    "<img src=\"img/_1.png\" width=\"20%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda**:\n",
    "1. Introduction to Spark\n",
    "1. Installing PySpark\n",
    "1. PySpark Basics\n",
    "1. PySpark and Pandas\n",
    "1. PySpark and SQL\n",
    "1. Load data from your DBMS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "**Big Data Challenge**:\n",
    "\n",
    "* Cost of storing data has dropped\n",
    "* The need for parallel computation has increased\n",
    "\n",
    "![IBM Blue Gene\\L](https://upload.wikimedia.org/wikipedia/commons/d/d3/IBM_Blue_Gene_P_supercomputer.jpg)\n",
    "\n",
    "**Note**: [IBM Blue Gen\\L](https://www.ibm.com/ibm/history/ibm100/us/en/icons/bluegene/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-14T09:06:51.596422Z",
     "start_time": "2020-06-14T09:06:51.588599Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What is [Apache Spark](https://spark.apache.org)**?\n",
    "\n",
    "> \"Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters\"\n",
    "\n",
    "[Chambers and Zaharia 2018](#references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Programming Languages Supported**:\n",
    "\n",
    "<img src=\"img/_0.png\" width=\"50%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-13T10:49:40.128171Z",
     "start_time": "2020-06-13T10:49:40.122611Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Spark's philosophy**:\n",
    "\n",
    "* *Unified*: Spark offers a large variety of data analytics tools\n",
    "* *Computing Engine*: Spark focuses on computing, not on storage\n",
    "* *Libraries*: Spark has different libraries to perform several tasks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Apache Spark Libraries**:\n",
    "\n",
    "* *Spark SQL*\n",
    "* *Spark Streaming*\n",
    "* *Spark MLlib*\n",
    "* *Spark GraphX*\n",
    "\n",
    "[Third-party projects](https://spark.apache.org/third-party-projects.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Spark Application**:\n",
    "\n",
    "| Component ||Role |\n",
    "|----|----|---|\n",
    "| *Spark Driver*| | Execute user-defined tasks |\n",
    "| *Cluster Manager* | | Manage workers nodes|\n",
    "| *Executors* | | Execute tasks |\n",
    "\n",
    "<img src=\"img/_5.png\" width=80%>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**From Python to Spark code and back**:\n",
    "\n",
    "<img src=\"img/_7.png\" width=80%>\n",
    "\n",
    "Source: _Bill Chambers, Matei Zaharia 2018_ (p. 23)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Installing PySpark\n",
    "\n",
    "There are several ways to set-up PySpark on your local machine. Here, two methods are discussed:\n",
    "* Pure-python users: \n",
    "```python\n",
    "pip install pyspark\n",
    "```\n",
    "* Conda users:\n",
    "```python\n",
    "conda install pyspark\n",
    "```\n",
    "Further info at [Spark Download page](https://spark.apache.org/downloads.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark - Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:42:06.797592Z",
     "start_time": "2020-06-19T10:42:06.726681Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#to create a spark session object\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# import datetime \n",
    "from datetime import date as dt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* More info on **Functions** at these [link-1](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) & [link-2](https://spark.apache.org/docs/2.3.0/api/sql/index.html#year)\n",
    "* More info on **Data Types** at this [link](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Opening a Session\n",
    "\n",
    "The **SparkSession** is a driver process that enables:\n",
    "\n",
    "* to control our Spark Application\n",
    "* to execute user-defined manipulations\n",
    "\n",
    "Check this [link](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) for further reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:43:44.101440Z",
     "start_time": "2020-06-19T10:43:36.024999Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# to open a Session\n",
    "spark = SparkSession.builder.appName('last_dance').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Spark UI**\n",
    "\n",
    "<img src=\"img/_6.png\" width=60%>\n",
    "\n",
    "The spark UI is useful to monitor your application. You have the following tabs:\n",
    "\n",
    "* *Jobs*: info concerning Spark jobs\n",
    "* *Stages*: info on individual stages and their tasks\n",
    "* *Storage*: info on data that is currently in our spark application\n",
    "* *Environment*: info on configurations and current settings of our application\n",
    "* *Executors*: info on the executors that run our application\n",
    "* *SQL*: refers to both SQL and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:44:27.071236Z",
     "start_time": "2020-06-19T10:44:27.037413Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Dataframe\n",
    "\n",
    "In order to create a dataframe from scratch, we need to:\n",
    "1. Create a schema, passing:\n",
    "  * Column names\n",
    "  * Data types\n",
    "1. Pass values as an array of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:48:18.891858Z",
     "start_time": "2020-06-19T10:48:16.949258Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Here, I define a schema\n",
    "# .add(field, data_type=None, nullable=True, metadata=None)\n",
    "\n",
    "schema = StructType().add(\"id\", \"integer\", True).add(\"first_name\", \"string\", True).add(\n",
    "    \"last_name\", \"string\", True).add(\"dob\", \"date\", True)\n",
    "\n",
    "'''\n",
    "schema = StructType().add(\"id\", IntegerType(), True).add(\"first_name\", StringType(), True).add(\n",
    "    \"last_name\", StringType(), True).add(\"dob\", DateType(), True)\n",
    "'''\n",
    "\n",
    "# Then, I can pass some values\n",
    "df = spark.createDataFrame([(1, 'Michael', \"Jordan\", dt(1963, 2, 17)),\n",
    "                            (2, 'Scottie', \"Pippen\", dt(1965, 9, 25)),\n",
    "                            (3, 'Dennis', \"Rodman\", dt(1961, 5, 16))],\n",
    "                           schema=schema)\n",
    "\n",
    "# Let's explore Schema structure\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T10:49:40.465738Z",
     "start_time": "2020-06-19T10:49:37.582205Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also leverage on functions to create a new column\n",
    "df=df.withColumn('age', F.year(F.current_date()) - F.year(df.dob))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Transformations**\n",
    "\n",
    "* Immutability: once created, data structures can not be changed\n",
    "* Lazy evaluation: computational instructions will be executed at the very last"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Actions**\n",
    "\n",
    "* view data\n",
    "* collect data\n",
    "* write to output data sources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark and Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load a csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Loading a csv file from you computer, you need to type:\n",
    "* Pands:\n",
    "  * db = pd.read_csv('path/to/movies.csv')\n",
    "* Pyspark:\n",
    "  * df = spark.read.csv('path/to/movies.csv', header=True, inferSchema=True)\n",
    "\n",
    "Here, we will import a csv directly from GitHub. Data are provided by [FiveThirtyEight](https://github.com/fivethirtyeight)\n",
    "\n",
    "[<img src=\"img/_2.png\" width=\"50%\">](https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:00:05.506049Z",
     "start_time": "2020-06-19T11:00:05.501696Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# import SparkFiles\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# target dataset\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/bechdel/movies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:01:27.578138Z",
     "start_time": "2020-06-19T11:01:21.652064Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading data with pandas\n",
    "db = pd.read_csv(url)\n",
    "\n",
    "# loading data with pyspark\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get('movies.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:00.801919Z",
     "start_time": "2020-06-19T08:55:00.785095Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas info\n",
    "db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:06.974456Z",
     "start_time": "2020-06-19T08:55:06.966256Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:07.890618Z",
     "start_time": "2020-06-19T08:55:07.861658Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas fetch 5\n",
    "db.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:12.386242Z",
     "start_time": "2020-06-19T08:55:11.952901Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark fetch 5\n",
    "df.show(5)\n",
    "\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:13.256545Z",
     "start_time": "2020-06-19T08:55:13.233172Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas filtering:\n",
    "db[db.year == 1970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:15.168109Z",
     "start_time": "2020-06-19T08:55:14.901104Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark filtering:\n",
    "df[df.year == 1970].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:16.211734Z",
     "start_time": "2020-06-19T08:55:16.201602Z"
    }
   },
   "outputs": [],
   "source": [
    "# get columns and data types\n",
    "print(\"\"\"\n",
    "Pandas db.columns:\n",
    "===================\n",
    "{}\n",
    "\n",
    "PySpark df.columns:\n",
    "===================\n",
    "{}\n",
    "\n",
    "Pandas db.dtype:\n",
    "===================\n",
    "{}\n",
    "\n",
    "PySpark df.dtypes:\n",
    "===================\n",
    "{}\n",
    "\n",
    "\"\"\".format(db.columns, df.columns, db.dtypes, df.dtypes), flush = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:19.717696Z",
     "start_time": "2020-06-19T08:55:19.671673Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas add a column\n",
    "db['newcol'] = db.domgross/db.intgross\n",
    "\n",
    "# pyspark add a column\n",
    "df=df.withColumn('newcol', df.domgross/df.intgross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:20.411358Z",
     "start_time": "2020-06-19T08:55:20.383463Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas rename columns\n",
    "db.rename(columns={'newcol': 'dgs/igs'}, inplace=True)\n",
    "\n",
    "# pyspark rename columns\n",
    "df=df.withColumnRenamed('newcol', 'dgs/igs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-06T08:44:37.205943Z",
     "start_time": "2020-06-06T08:44:37.202051Z"
    }
   },
   "source": [
    "## Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:22.433936Z",
     "start_time": "2020-06-19T08:55:22.395441Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas drop `code' column\n",
    "db.drop('code', axis=1, inplace=True)\n",
    "\n",
    "# pyspark drop `code' column\n",
    "df=df.drop('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:23.110718Z",
     "start_time": "2020-06-19T08:55:23.076353Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas dropna()\n",
    "db.dropna(subset=['domgross'], inplace=True)\n",
    "\n",
    "# pyspark dropna()\n",
    "df=df.dropna(subset='domgross')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:25.772431Z",
     "start_time": "2020-06-19T08:55:25.717212Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas describe\n",
    "db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:55:28.562107Z",
     "start_time": "2020-06-19T08:55:27.881541Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark describe\n",
    "df.describe(['year', 'budget']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T10:17:13.644915Z",
     "start_time": "2020-06-10T10:17:13.638567Z"
    }
   },
   "source": [
    "# Pyspark and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:08:57.097001Z",
     "start_time": "2020-06-19T11:08:57.061368Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark rename 'budget_2013$'\n",
    "df=df.withColumnRenamed('budget_2013$', 'budget_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:10:17.413884Z",
     "start_time": "2020-06-19T11:10:16.590406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary table \n",
    "df.createOrReplaceTempView('bechdel')\n",
    "\n",
    "# Run a simple SQL command\n",
    "sql = spark.sql(\"\"\"SELECT imdb, year, title, budget FROM bechdel LIMIT(5)\"\"\")\n",
    "sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:14:53.890743Z",
     "start_time": "2020-06-19T11:14:51.360644Z"
    }
   },
   "outputs": [],
   "source": [
    "# AVG budget differences\n",
    "sql_avg = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "    binary, \n",
    "    COUNT(*) AS count, \n",
    "    format_number(AVG(budget),2) AS avg_budget, \n",
    "    format_number((SELECT AVG(budget) FROM bechdel),2) AS avg_budget_samp,\n",
    "    format_number(AVG(budget_2013),2) AS avg_budget2013,\n",
    "    format_number((SELECT AVG(budget_2013) FROM bechdel),2) AS avg_budget2013_samp\n",
    "    FROM bechdel GROUP BY binary\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sql_avg.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from DBMS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the following you need to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:23:01.648047Z",
     "start_time": "2020-06-19T11:23:01.583515Z"
    }
   },
   "outputs": [],
   "source": [
    "# to create a spark session object\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with postgre you need to:\n",
    "    \n",
    "* Download the *postgresql-42.7.3.jar file* [here](https://jdbc.postgresql.org/download/)\n",
    "* Include the path to the downloaded jar file into SparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:18:43.342799Z",
     "start_time": "2020-06-19T11:18:38.676180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open a session running data from PostgreSQL\n",
    "spark_postgre = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"last_dance_postgre\") \\\n",
    "    .config(\"spark.jars\", \"/Users/matteodevigili/share/py4j/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:18:44.820975Z",
     "start_time": "2020-06-19T11:18:44.797346Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_postgre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:59:40.193615Z",
     "start_time": "2020-06-22T15:59:40.045141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from PostgreSQL running at localhost\n",
    "df = spark_postgre.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5434/pagila\") \\\n",
    "    .option(\"dbtable\", \"film\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"smm695\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:21:08.771475Z",
     "start_time": "2020-06-19T11:21:05.732524Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some stats\n",
    "df.describe(['release_year', 'rental_rate', 'rental_duration']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:22:06.426641Z",
     "start_time": "2020-06-19T11:22:05.835428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary table \n",
    "df.createOrReplaceTempView('film')\n",
    "\n",
    "# Run a simple SQL command\n",
    "sql = spark_postgre.sql(\"\"\"SELECT title, release_year, length, rating FROM film LIMIT(1)\"\"\")\n",
    "sql.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reference check the [Python Guide provided by Mongo](https://docs.mongodb.com/spark-connector/current/python-api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:25:16.920144Z",
     "start_time": "2020-06-19T11:25:10.141682Z"
    }
   },
   "outputs": [],
   "source": [
    "# add path to Mongo\n",
    "spark_mongo = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"last_dance_mongo\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/amazon.music\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/amazon.music\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:25:21.410330Z",
     "start_time": "2020-06-19T11:25:21.375456Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:26:05.596318Z",
     "start_time": "2020-06-19T11:26:02.191328Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data from MongoDB\n",
    "df = spark_mongo.read.format(\"mongo\").load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:26:26.487659Z",
     "start_time": "2020-06-19T11:26:20.795549Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some stats\n",
    "df.describe(['overall', 'unixReviewTime']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T11:27:28.119177Z",
     "start_time": "2020-06-19T11:27:16.008431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary table \n",
    "df.createOrReplaceTempView('music')\n",
    "\n",
    "# Run a simple SQL command\n",
    "sql = spark_mongo.sql(\"\"\"SELECT asin, date, helpful, overall, unixReviewTime FROM music LIMIT(1)\"\"\")\n",
    "sql.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Bill Chambers, Matei Zaharia 2018,[\"Spark: The Definitive Guide\"](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) \n",
    "\n",
    "<img src=\"img/_3.png\" width=\"20%\">\n",
    "\n",
    "* Pramod Singh 2019, [\"Learn PySpark: Build Python-based Machine Learning and Deep Learning Models\n",
    "\"](https://www.ibs.it/learn-pyspark-build-python-based-libro-inglese-pramod-singh/e/9781484249604) \n",
    "\n",
    "<img src=\"img/_4.png\" width=\"18%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "198px",
    "left": "1089px",
    "right": "20px",
    "top": "120px",
    "width": "331px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "116c845e412a2c4626cafe8646f2e306d5b9b3ff9451d4010b0ab6cc92d61267"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
