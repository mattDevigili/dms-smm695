{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    ">1. DBMS: Read and Write\n",
    ">   * MongoDB\n",
    ">   * PostgreSQL\n",
    ">2. Preprocessing and Regression\n",
    ">   * Initialize Spark with both Mongo and Postgre\n",
    ">   * Prepare data\n",
    ">   * Logistic Regression\n",
    ">   * Linear Regression\n",
    ">3. Further examples\n",
    ">   * Pipeline \n",
    ">   * Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:45:35.594694Z",
     "start_time": "2020-06-26T15:45:35.529017Z"
    }
   },
   "outputs": [],
   "source": [
    "# to create a spark session object\n",
    "from pyspark.sql import SparkSession\n",
    "# data types\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBMS: Read and Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Mods:**\n",
    "\n",
    " [<img src=\"images/_1.png\" width=70%>](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:19:53.059591Z",
     "start_time": "2020-06-26T15:19:42.078330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Session with Mongo\n",
    "spark_mongo = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"music_mongo\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/amazon.music\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/amazon.msample\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:19:55.157757Z",
     "start_time": "2020-06-26T15:19:55.124820Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:21:18.493071Z",
     "start_time": "2020-06-26T15:21:18.488169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create an aggregation pipeline\n",
    "pipeline = \"[{$match: {'overall': {'$gt':1,'$lt':5}}}, {'$unset':['helpful', 'reviewTime', ]}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:22:09.990762Z",
     "start_time": "2020-06-26T15:22:04.602515Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data from MongoDB\n",
    "df = spark_mongo.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:22:58.457015Z",
     "start_time": "2020-06-26T15:22:28.777838Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some stats\n",
    "df.describe(['overall', 'unixReviewTime']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:23:57.612687Z",
     "start_time": "2020-06-26T15:23:57.590113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a sample\n",
    "# sample(withReplacement=None, fraction=None, seed=None)\n",
    "df=df.sample(0.01, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:25:37.221541Z",
     "start_time": "2020-06-26T15:25:36.796396Z"
    }
   },
   "outputs": [],
   "source": [
    "# to tokenize\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(outputCol=\"tokenized\", inputCol=\"reviewText\")\n",
    "df=tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:27:13.111049Z",
     "start_time": "2020-06-26T15:27:00.728103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's save \n",
    "df.write.format(\"mongo\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:32:46.941764Z",
     "start_time": "2020-06-26T15:32:41.127905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open a session with Postgre\n",
    "spark_postgre = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"tate_postgre\") \\\n",
    "    .config(\"spark.jars\", \"/Users/matteodevigili/GitHub/dms-smm695/.venv/share/py4j/postgresql-42.4.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:32:49.044991Z",
     "start_time": "2020-06-26T15:32:49.009180Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_postgre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"images/_0.png\" width=90%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:35:17.686194Z",
     "start_time": "2020-06-26T15:35:14.151459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from PostgreSQL running at localhost\n",
    "params = spark_postgre.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/tate\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"smm695\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "\n",
    "df_0 = params \\\n",
    "    .option(\"dbtable\", \"artworks_id\") \\\n",
    "    .load()\n",
    "\n",
    "df_1 = params \\\n",
    "    .option(\"dbtable\", \"artworks\") \\\n",
    "    .load()\n",
    "\n",
    "df_2 = params \\\n",
    "    .option(\"dbtable\", \"artists\") \\\n",
    "    .load()\n",
    "\n",
    "df_3 = params \\\n",
    "    .option(\"dbtable\", \"roles\") \\\n",
    "    .load()\n",
    "\n",
    "df_0.printSchema()\n",
    "df_1.printSchema()\n",
    "df_2.printSchema()\n",
    "df_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:37:14.892706Z",
     "start_time": "2020-06-26T15:36:57.445594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary tables\n",
    "df_0.createOrReplaceTempView('artworks_id')\n",
    "df_1.createOrReplaceTempView('artworks')\n",
    "df_2.createOrReplaceTempView('artists')\n",
    "df_3.createOrReplaceTempView('roles')\n",
    "\n",
    "\n",
    "df = spark_postgre.sql(\"\"\"\n",
    "SELECT title, year, artist, artistrole\n",
    "FROM artworks aws\n",
    "JOIN artworks_id aid ON aid.accession_number = aws.accession_number\n",
    "JOIN artists as ON as.artistid = aid.artistid\n",
    "JOIN roles r ON r.role_id = aid.role_id\n",
    "WHERE year IS NOT NULL\"\"\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:39:13.567000Z",
     "start_time": "2020-06-26T15:38:52.067959Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexer\n",
    "indexer = StringIndexer() \\\n",
    "    .setInputCol(\"artistrole\") \\\n",
    "    .setOutputCol(\"artistrole_index\")\n",
    "\n",
    "# fit the indexer\n",
    "fitted = indexer.fit(df)\n",
    "\n",
    "# modify data\n",
    "df = fitted.transform(df)\n",
    "\n",
    "# show five\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:41:16.607179Z",
     "start_time": "2020-06-26T15:41:05.316015Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the table\n",
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://localhost:5432/tate\"\n",
    "properties = {\"user\": \"postgres\",\"password\": \"smm695\",\"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df.write.jdbc(url=url, table=\"test\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative script**\n",
    "```python\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/tate\") \\\n",
    "    .option(\"dbtable\", \"test\") \\\n",
    "    .option(\"user\", \"dms695\") \\\n",
    "    .option(\"password\", \"smm695\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "```\n",
    "\n",
    "**Check on psql or PgAdmin**\n",
    "\n",
    "```sql\n",
    "SELECT DISTINCT artistrole, artistrole_index FROM test \n",
    "ORDER BY artistrole_index;\n",
    "```\n",
    "\n",
    "_Expected result:_\n",
    "\n",
    "|artistrole\t| artistrole_index |\n",
    "| --- | --- |\n",
    "|artist\t| 0 |\n",
    "|after\t| 1 |\n",
    "|attributed to\t| 2 |\n",
    "|prints after\t| 3 |\n",
    "|formerly attributed to\t| 4 | \n",
    "|manner of\t| 5 |\n",
    "|and assistants\t| 6 |\n",
    "|pupil of\t| 7 |\n",
    "|and other artists\t| 8 |\n",
    "|circle of\t| 9 |\n",
    "|follower of\t| 10 |\n",
    "|imitator of\t| 11 |\n",
    "|studio of\t| 12| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create a spark session object\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# data types\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:49:05.172374Z",
     "start_time": "2020-06-26T15:48:58.169547Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open a session\n",
    "spark_session = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"analysis\") \\\n",
    "    .config(\"spark.jars\", \"/Users/matteodevigili/GitHub/dms-smm695/.venv/share/py4j/postgresql-42.4.0.jar\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:49:05.390412Z",
     "start_time": "2020-06-26T15:49:05.345796Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:50:53.086763Z",
     "start_time": "2020-06-26T15:50:47.263628Z"
    }
   },
   "outputs": [],
   "source": [
    "# import SparkFiles\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# target dataset\n",
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/bechdel/movies.csv'\n",
    "\n",
    "# loading data with pyspark\n",
    "spark_session.sparkContext.addFile(url)\n",
    "df = spark_session.read.csv(SparkFiles.get('movies.csv'), header=True, inferSchema=True)\n",
    "\n",
    "# let's print the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:51:44.833792Z",
     "start_time": "2020-06-26T15:51:44.806464Z"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark rename 'budget_2013$'\n",
    "df=df.withColumnRenamed('budget_2013$', 'budget_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:52:09.478857Z",
     "start_time": "2020-06-26T15:52:07.812073Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(['year', 'clean_test', 'binary', 'budget_2013', 'domgross_2013$', 'intgross_2013$']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:53:00.213964Z",
     "start_time": "2020-06-26T15:53:00.127865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cast values to int\n",
    "df = df.withColumn(\"domgross_2013\", df[\"domgross_2013$\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"intgross_2013\", df[\"intgross_2013$\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:54:03.898361Z",
     "start_time": "2020-06-26T15:54:03.860871Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop null values\n",
    "df=df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:55:59.073494Z",
     "start_time": "2020-06-26T15:55:56.581825Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's inspect data\n",
    "hist = df.select(['year','budget_2013', \"domgross_2013\", \"intgross_2013\"\n",
    "                  ]) \\\n",
    "         .sample(False, 0.5, 123) \\\n",
    "         .toPandas() \\\n",
    "         .hist(bins=20,figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:56:48.872831Z",
     "start_time": "2020-06-26T15:56:45.923646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's inspect string data\n",
    "df.groupby('clean_test').count().show()\n",
    "df.groupby('binary').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T15:57:58.140011Z",
     "start_time": "2020-06-26T15:57:56.037100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying some transformations: String indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Indexing 'clean_test'\n",
    "idx_0 = StringIndexer().setInputCol(\"clean_test\").setOutputCol(\"clean_test_idx\")\n",
    "\n",
    "# Indexing 'binary'\n",
    "idx_1 = StringIndexer() \\\n",
    "        .setInputCol(\"binary\") \\\n",
    "        .setOutputCol(\"binary_idx\")\n",
    "\n",
    "# Applying to df\n",
    "df = idx_0.fit(df).transform(df)\n",
    "df = idx_1.fit(df).transform(df)\n",
    "\n",
    "# Inspect result\n",
    "df.select(['binary', 'binary_idx','clean_test', 'clean_test_idx']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:00:13.616531Z",
     "start_time": "2020-06-26T16:00:13.063601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step two: One-Hot Encoding\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Encoding 'clean_test'\n",
    "ohe_0 = OneHotEncoder().setInputCol(\"clean_test_idx\").setOutputCol(\"clean_test_ohe\")\n",
    "\n",
    "# Encoding 'binary'\n",
    "ohe_1 = OneHotEncoder().setInputCol(\"binary_idx\").setOutputCol(\"binary_ohe\")\n",
    "\n",
    "# Let's show what we have\n",
    "ohe_0.fit(df).transform(df).select(['clean_test', 'clean_test_idx', 'clean_test_ohe']).show()\n",
    "ohe_1.fit(df).transform(df).select(['binary', 'binary_idx', 'binary_ohe']).show()\n",
    "\n",
    "# Apply ohe_1 to df\n",
    "df = ohe_1.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:02:59.567743Z",
     "start_time": "2020-06-26T16:02:59.190846Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assembling a Vector for Logistic Regression\n",
    "v_0 = VectorAssembler() \\\n",
    "     .setInputCols([\"year\", \"budget_2013\", \"domgross_2013\", \"intgross_2013\"]) \\\n",
    "     .setOutputCol('features_0')\n",
    "\n",
    "# Assembling a Vector for Linear Regression\n",
    "v_1 = VectorAssembler() \\\n",
    "     .setInputCols([\"year\", \"budget_2013\", \"domgross_2013\", \"binary_ohe\"]) \\\n",
    "     .setOutputCol('features_1')\n",
    "\n",
    "# Applying\n",
    "df = v_0.transform(df)\n",
    "df = v_1.transform(df)\n",
    "\n",
    "# show\n",
    "df.select(['features_0', 'features_1']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:03:39.689849Z",
     "start_time": "2020-06-26T16:03:39.639463Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting training and test\n",
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:05:08.943841Z",
     "start_time": "2020-06-26T16:05:08.770778Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instance of Logistic Regression\n",
    "lr_0 = LogisticRegression(labelCol=\"binary_idx\",featuresCol=\"features_0\")\n",
    "\n",
    "# let's inspect the parameters\n",
    "a = lr_0.explainParams().split('\\n')\n",
    "x = 1\n",
    "for i in a:\n",
    "    b = i.split(':',1)\n",
    "    b_0,b_1 = '\\033[1m' + b[0] + '\\033[0m', b[1]\n",
    "    print(\"\"\"{}. {} : {}\n",
    "    \"\"\".format(x, b_0,b_1), flush=True)\n",
    "    x=x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:06:33.192123Z",
     "start_time": "2020-06-26T16:06:07.198686Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's fit data\n",
    "fitLr_0 = lr_0.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"\"\" \n",
    "\n",
    "Coefficients:\n",
    "============\n",
    "{}\n",
    "\n",
    "\n",
    "Intercept:\n",
    "=========\n",
    "{}\n",
    "\n",
    "\"\"\".format(fitLr_0.coefficients, fitLr_0.intercept), flush=True)\n",
    "\n",
    "# comparing binary and prediction\n",
    "fitLr_0.transform(train).select(\"binary_idx\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:07:24.777692Z",
     "start_time": "2020-06-26T16:07:23.917628Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some more info\n",
    "s_0 = fitLr_0.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = s_0.objectiveHistory\n",
    "\n",
    "print(\"\"\"\n",
    "- Accuracy: {}\n",
    "- Area Under ROC : {}\n",
    "- False Positive Rate by Label: {}\n",
    "- Precision by Label: {}\n",
    "- Tot. Iterations: {}\n",
    "- Objective History: \n",
    "{}\n",
    "\"\"\".format(s_0.accuracy, s_0.areaUnderROC,\n",
    "           s_0.falsePositiveRateByLabel, s_0.precisionByLabel,\n",
    "           s_0.totalIterations, [obj for obj in objectiveHistory]),\n",
    "      flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:09:37.876813Z",
     "start_time": "2020-06-26T16:09:35.889340Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# instance of Linear Regression\n",
    "lr_1 = LinearRegression(labelCol=\"intgross_2013\",featuresCol=\"features_1\")\n",
    "\n",
    "# let's inspect parameters\n",
    "a = lr_1.explainParams().split('\\n')\n",
    "x = 1\n",
    "for i in a:\n",
    "    b = i.split(':',1)\n",
    "    b_0,b_1 = '\\033[1m' + b[0] + '\\033[0m', b[1]\n",
    "    print(\"\"\"{}. {} : {}\n",
    "    \"\"\".format(x, b_0,b_1), flush=True)\n",
    "    x=x+1\n",
    "\n",
    "# fit\n",
    "fitLr_1 = lr_1.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:09:54.128563Z",
     "start_time": "2020-06-26T16:09:54.008378Z"
    }
   },
   "outputs": [],
   "source": [
    "# summary stats\n",
    "s_1 = fitLr_1.summary\n",
    "print(\"\"\"\n",
    "- R^2 adj: {}\n",
    "- RMSE : {}\n",
    "- Intercept: {}\n",
    "- Coefficients: {}\n",
    "- p-values: {}\n",
    "\"\"\".format(s_1.r2adj, s_1.rootMeanSquaredError, fitLr_1.intercept, fitLr_1.coefficients, s_1.pValues),\n",
    "      flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:11:12.930912Z",
     "start_time": "2020-06-26T16:11:12.236514Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's create a reference dataframe\n",
    "results = fitLr_1.transform(train).select(['intgross_2013', 'prediction'])\n",
    "results.createOrReplaceTempView('results')\n",
    "spark_session.sql(\n",
    "    \"SELECT format_number(intgross_2013,2) as intgross, format_number(prediction,2) as prediction FROM results\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:13:05.140067Z",
     "start_time": "2020-06-26T16:13:04.515287Z"
    }
   },
   "outputs": [],
   "source": [
    "# save to Postgre\n",
    "results.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/smm695\") \\\n",
    "    .option(\"dbtable\", \"results\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"smm695\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:13:41.947082Z",
     "start_time": "2020-06-26T16:13:41.159744Z"
    }
   },
   "outputs": [],
   "source": [
    "# save to MongoDB\n",
    "results.write \\\n",
    "        .option(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/pyspark.results\") \\\n",
    "        .format(\"mongo\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:15:36.694299Z",
     "start_time": "2020-06-26T16:15:36.689034Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:18:36.683753Z",
     "start_time": "2020-06-26T16:18:36.616591Z"
    }
   },
   "outputs": [],
   "source": [
    "# R Formula to perform simple preprocessing\n",
    "rf = RFormula()\n",
    "\n",
    "# logistic regression\n",
    "lr = LogisticRegression().setLabelCol(\"binary_idx\").setFeaturesCol(\"features\")\n",
    "\n",
    "# let's create a pipeline\n",
    "pipeline = Pipeline().setStages([rf, lr])\n",
    "\n",
    "# setting some parameters\n",
    "params = ParamGridBuilder()\\\n",
    "         .addGrid(rf.formula, \n",
    "                  [\"binary_idx ~ budget_2013\",\n",
    "                   \"binary_idx ~ year + budget_2013\", \n",
    "                   \"binary_idx ~ year + budget_2013 + domgross_2013 + intgross_2013\"]) \\\n",
    "         .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "         .addGrid(lr.regParam, [0.0, 0.01, 0.1]) \\\n",
    "         .build()\n",
    "\n",
    "# evaluation\n",
    "evaluator = BinaryClassificationEvaluator() \\\n",
    "            .setMetricName(\"areaUnderROC\") \\\n",
    "            .setRawPredictionCol(\"prediction\") \\\n",
    "            .setLabelCol(\"label\")\n",
    "\n",
    "# hyperparameter tuning\n",
    "tvs = TrainValidationSplit() \\\n",
    "      .setTrainRatio(0.75) \\\n",
    "      .setEstimatorParamMaps(params) \\\n",
    "      .setEstimator(pipeline) \\\n",
    "      .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:20:11.511752Z",
     "start_time": "2020-06-26T16:18:58.044165Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit \n",
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:20:37.760085Z",
     "start_time": "2020-06-26T16:20:37.748205Z"
    }
   },
   "outputs": [],
   "source": [
    "# get info\n",
    "l = []\n",
    "x = 1\n",
    "for i in tvsFitted.getEstimatorParamMaps():\n",
    "    for a in i.keys():\n",
    "        l.append(a)\n",
    "    b, c, d = i.get(l[0]), i.get(l[1]), i.get(l[2])\n",
    "    print(\"\"\"\n",
    "    {}. \\033[1mModel\\033[0m: {} \\033[1mElasticNet\\033[0m: {} \\033[1mRegular.\\033[0m: {}\"\"\".format(x, b, c, d))\n",
    "    x = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:21:45.070524Z",
     "start_time": "2020-06-26T16:21:45.057970Z"
    }
   },
   "outputs": [],
   "source": [
    "# best model info\n",
    "bs_0 = tvsFitted.bestModel.stages[0].extractParamMap()\n",
    "bs_1 = tvsFitted.bestModel.stages[1].extractParamMap()\n",
    "\n",
    "l = []\n",
    "m = []\n",
    "for i in bs_0.keys():\n",
    "    l.append(i)\n",
    "for i in bs_1.keys():\n",
    "    m.append(i)\n",
    "        \n",
    "print(\"\"\"\n",
    "Model: {} \n",
    "\n",
    "ElasticNet: {} \n",
    "\n",
    "Regularization: {}\n",
    "\"\"\".format(bs_0.get(l[-1]), bs_1.get(m[1]), bs_1.get(m[-4])), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:22:19.873676Z",
     "start_time": "2020-06-26T16:22:19.287086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Best Model\n",
    "s = tvsFitted.bestModel.stages[1].summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = s.objectiveHistory\n",
    "\n",
    "print(\"\"\"\n",
    "- Accuracy: {}\n",
    "- Area Under ROC : {}\n",
    "- False Positive Rate by Label: {}\n",
    "- Precision by Label: {}\n",
    "- Tot. Iterations: {}\n",
    "- Objective History: \n",
    "{}\n",
    "\"\"\".format(s.accuracy, s.areaUnderROC,\n",
    "           s.falsePositiveRateByLabel, s.precisionByLabel,\n",
    "           s.totalIterations, [obj for obj in objectiveHistory]),\n",
    "      flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T16:22:39.388820Z",
     "start_time": "2020-06-26T16:22:39.017532Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's see how it performs on the test\n",
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import format_number as fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reload the music collection\n",
    "df = spark_session \\\n",
    "    .read \\\n",
    "    .format(\"mongo\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/amazon.music\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample\n",
    "df=df.sample(False, 0.001, 123)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "tokenizer = RegexTokenizer(outputCol=\"DOC_TOKEN\", inputCol=\"reviewText\", toLowercase=True, pattern=\"\\\\W\")\n",
    "df=tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "\n",
    "```python\n",
    "print(df.first()['reviewText'][0:503], '\\n')\n",
    "print(*df.first()['DOC_TOKEN'][0:100], sep = \", \")\n",
    "```\n",
    "\n",
    ">Instead of The Doors Collections this set should have been called 3 for 1, a joking reference to the song 5 to 1. This DVD is three previously released videos from the 80's and early 90's, Dance On Fire, Live At The Hollywood Bowl, and The Soft Parade.Dance On Fire is a slow start for the DVD, it is formatted like a 60's era LP, a collection of songs strung together that may or may not have a connection to each other. Jim Morrison once suggested that between tracks on a Doors album they should put  \n",
    ">\n",
    ">instead, of, the, doors, collections, this, set, should, have, been, called, 3, for, 1, a, joking, reference, to, the, song, 5, to, 1, this, dvd, is, three, previously, released, videos, from, the, 80, s, and, early, 90, s, dance, on, fire, live, at, the, hollywood, bowl, and, the, soft, parade, dance, on, fire, is, a, slow, start, for, the, dvd, it, is, formatted, like, a, 60, s, era, lp, a, collection, of, songs, strung, together, that, may, or, may, not, have, a, connection, to, each, other, jim, morrison, once, suggested, that, between, tracks, on, a, doors, album, they, should, put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also remove stopwords:**\n",
    "\n",
    "```python\n",
    "# let's remove stopwords\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"DOC_TOKEN\", outputCol=\"DOC_STOP\")\n",
    "df=remover.transform(df)\n",
    "\n",
    "# let's inspect one review \n",
    "print(*df.first()['DOC_STOP'][0:100], sep = \", \")\n",
    "```\n",
    "> instead, doors, collections, set, called, 3, 1, joking, reference, song, 5, 1, dvd, three, previously, released, videos, 80, early, 90, dance, fire, live, hollywood, bowl, soft, parade, dance, fire, slow, start, dvd, formatted, like, 60, era, lp, collection, songs, strung, together, may, may, connection, jim, morrison, suggested, tracks, doors, album, put, poems, exactly, happens, video, poem, morrison, video, rare, rarely, seen, elektra, promotional, film, break, others, created, 80, directed, ray, manzarek, specific, intention, video, shown, mtv, wild, child, surely, centerpiece, meant, l, woman, l, woman, almost, entirely, shot, ray, without, vintage, doors, footage, mini, drama, prostitute, serial, killer, john, doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# parameters\n",
    "word2Vec = Word2Vec(vectorSize=100, \n",
    "                    seed=123, \n",
    "                    maxIter=10, \n",
    "                    inputCol=\"DOC_TOKEN\", \n",
    "                    outputCol=\"model\")\n",
    "\n",
    "# fit the model\n",
    "model = word2Vec.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the vectors\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect some synonyms\n",
    "model.findSynonyms(\"album\", 5).select(\"word\", fmt(\"similarity\", 5).alias(\"similarity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "116c845e412a2c4626cafe8646f2e306d5b9b3ff9451d4010b0ab6cc92d61267"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
